# QA Bot
HERE ALL THE TEST
https://colab.research.google.com/drive/1WJx042KBQzC9lzLhKMK-F-l3RSUHnU_A?usp=sharing



Resources
Read or watch:

Improving Language Understanding by Generative Pre-Training (2018)
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)
SQuAD 2.0
Know What You Donâ€™t Know: Unanswerable Questions for SQuAD (2018)
GLUE Benchmark
GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding (2019)
Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition (2018)
More recent papers in NLP:

Generating Long Sequences with Sparse Transformers (2019)
Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (2019)
XLNet: Generalized Autoregressive Pretraining for Language Understanding (2019)
Language Models are Unsupervised Multitask Learners (GPT-2, 2019)
Language Models are Few-Shot Learners (GPT-3, 2020)
ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations (2020)
To keep up with the newest papers and their code bases go to paperswithcode.com. For example, check out the raked list of state of the art models for Language Modelling on Penn Treebank.

General
What is Question-Answering?
What is Semantic Search?
What is BERT?
How to develop a QA chatbot
How to use the transformers library
How to use the tensorflow-hub library


